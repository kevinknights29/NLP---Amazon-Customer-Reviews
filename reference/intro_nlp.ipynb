{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "baUdqhMHiJ1V"
   },
   "source": [
    "# Introducción a NLP \n",
    "\n",
    "Para ilustrar el análisis de sentimiento vamos a analizar datos de revisiones de películas los cuales incluyen 25,000 datos en el training y 25,000 datos en el test. Para más información pueden revisar el siguiente sitio web:                                    \n",
    "http://ai.stanford.edu/~amaas/data/sentiment/\n",
    "\n",
    "## Descargando los datos\n",
    "\n",
    "Los siguientes son comandos de linux para descargar los datos.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ADnq1VZKjg9Z",
    "outputId": "7ef5894f-014d-447b-957e-aa5c4b047fbc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: cannot create directory ‘../data’: File exists\n",
      "--2021-11-27 02:48:12--  http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
      "Resolving ai.stanford.edu (ai.stanford.edu)... 171.64.68.10\n",
      "Connecting to ai.stanford.edu (ai.stanford.edu)|171.64.68.10|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 84125825 (80M) [application/x-gzip]\n",
      "Saving to: ‘../data/aclImdb_v1.tar.gz’\n",
      "\n",
      "../data/aclImdb_v1. 100%[===================>]  80.23M  39.0MB/s    in 2.1s    \n",
      "\n",
      "2021-11-27 02:48:15 (39.0 MB/s) - ‘../data/aclImdb_v1.tar.gz’ saved [84125825/84125825]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%mkdir ../data\n",
    "!wget -O ../data/aclImdb_v1.tar.gz http://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "!tar -zxf ../data/aclImdb_v1.tar.gz -C ../data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "034w78r3kTDQ"
   },
   "source": [
    "## Preparando los datos\n",
    "\n",
    "A continuación vamos a consolidar la información ya que los sentimientos para cada pelicula estan divididos en varios archivos como lo muestran los siguientes comandos de linux."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rc2cIu_Tj65T",
    "outputId": "3dc6acaa-e656-47db-c29e-30faf31d2b32"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "tree is already the newest version (1.7.0-5).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 37 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!apt-get install tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cibGTf-flKms",
    "outputId": "5fcc7c3f-bf2e-482f-8941-0838bd8be29c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data\n",
      "├── aclImdb\n",
      "│   ├── imdbEr.txt\n",
      "│   ├── imdb.vocab\n",
      "│   ├── README\n",
      "│   ├── test\n",
      "│   │   ├── labeledBow.feat\n",
      "│   │   ├── neg [12500 entries exceeds filelimit, not opening dir]\n",
      "│   │   ├── pos [12500 entries exceeds filelimit, not opening dir]\n",
      "│   │   ├── urls_neg.txt\n",
      "│   │   └── urls_pos.txt\n",
      "│   └── train\n",
      "│       ├── labeledBow.feat\n",
      "│       ├── neg [12500 entries exceeds filelimit, not opening dir]\n",
      "│       ├── pos [12500 entries exceeds filelimit, not opening dir]\n",
      "│       ├── unsup [50000 entries exceeds filelimit, not opening dir]\n",
      "│       ├── unsupBow.feat\n",
      "│       ├── urls_neg.txt\n",
      "│       ├── urls_pos.txt\n",
      "│       └── urls_unsup.txt\n",
      "└── aclImdb_v1.tar.gz\n",
      "\n",
      "8 directories, 12 files\n"
     ]
    }
   ],
   "source": [
    "!tree --filelimit=50  ../data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wW_NjZnnlR_9",
    "outputId": "86017c25-2d79-4c93-c277-ac2ae823cbf9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0_9.txt\n",
      "10000_8.txt\n",
      "10001_10.txt\n",
      "10002_7.txt\n",
      "10003_8.txt\n",
      "10004_8.txt\n",
      "10005_7.txt\n",
      "10006_7.txt\n",
      "10007_7.txt\n",
      "10008_7.txt\n"
     ]
    }
   ],
   "source": [
    "!ls ../data/aclImdb/train/pos/ | head "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QGmxmcJhnHoT",
    "outputId": "4dee0619-f030-47df-ef5d-73064561191e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!"
     ]
    }
   ],
   "source": [
    "!cat ../data/aclImdb/train/pos/0_9.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1wW2YwmQnYjM"
   },
   "source": [
    "Con el siguiente código almacenaremos la información de las revisiones de las peliculas y sus polaridades en diccionarios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "dn4O1qwOnNpG"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "def read_imdb_data(data_dir='../data/aclImdb'):\n",
    "    data = {}\n",
    "    labels = {}\n",
    "    \n",
    "    for data_type in ['train', 'test']:\n",
    "        data[data_type] = {}\n",
    "        labels[data_type] = {}\n",
    "        \n",
    "        for sentiment in ['pos', 'neg']:\n",
    "            data[data_type][sentiment] = []\n",
    "            labels[data_type][sentiment] = []\n",
    "            \n",
    "            path = os.path.join(data_dir, data_type, sentiment, '*.txt')\n",
    "            files = glob.glob(path)\n",
    "            \n",
    "            for f in files:\n",
    "                with open(f) as review:\n",
    "                    data[data_type][sentiment].append(review.read())\n",
    "                    # Here we represent a positive review by '1' and a negative review by '0'\n",
    "                    labels[data_type][sentiment].append(1 if sentiment == 'pos' else 0)\n",
    "                    \n",
    "            assert len(data[data_type][sentiment]) == len(labels[data_type][sentiment]), \\\n",
    "                    \"{}/{} data size does not match labels size\".format(data_type, sentiment)\n",
    "                \n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xHGQbNPRrU6t",
    "outputId": "25f1653c-51d3-4be4-8c74-12b31557c551"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB reviews: train = 12500 pos / 12500 neg, test = 12500 pos / 12500 neg\n"
     ]
    }
   ],
   "source": [
    "data, labels = read_imdb_data()\n",
    "print(\"IMDB reviews: train = {} pos / {} neg, test = {} pos / {} neg\".format(\n",
    "            len(data['train']['pos']), len(data['train']['neg']),\n",
    "            len(data['test']['pos']), len(data['test']['neg'])))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6e-fGYqnr5C7"
   },
   "source": [
    "Con los siguientes códigos volvemos a procesar los datos de tal manera que vamos a consolidar la información de los reviews y los labels en listas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "tDECWUKRrboQ"
   },
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "def prepare_imdb_data(data, labels):\n",
    "    \"\"\"Prepare training and test sets from IMDb movie reviews.\"\"\"\n",
    "    \n",
    "    #Combine positive and negative reviews and labels\n",
    "    data_train = data['train']['pos'] + data['train']['neg']\n",
    "    data_test = data['test']['pos'] + data['test']['neg']\n",
    "    labels_train = labels['train']['pos'] + labels['train']['neg']\n",
    "    labels_test = labels['test']['pos'] + labels['test']['neg']\n",
    "    \n",
    "    #Shuffle reviews and corresponding labels within training and test sets\n",
    "    data_train, labels_train = shuffle(data_train, labels_train)\n",
    "    data_test, labels_test = shuffle(data_test, labels_test)\n",
    "    \n",
    "    # Return a unified training data, test data, training labels, test labets\n",
    "    return data_train, data_test, labels_train, labels_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YjVWFqN_r7uw",
    "outputId": "6d539fe9-2d6f-44bc-cb82-fbf4e5d67ad4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDb reviews (combined): train = 25000, test = 25000\n"
     ]
    }
   ],
   "source": [
    "train_X, test_X, train_y, test_y = prepare_imdb_data(data, labels)\n",
    "print(\"IMDb reviews (combined): train = {}, test = {}\".format(len(train_X), len(test_X)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 139
    },
    "id": "jyBnjZj7r_3a",
    "outputId": "b898b9d1-8b47-4ad1-9b59-e381d75f52eb"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "\"Let me being by saying the I followed watching this video by watching Saw and after Bleed, Saw looked like the all time greatest horror flick ever even though I thought it was only fairly good. Bleed is pretty bad. The best part is seeing the female cast nude. The gore is very fake looking and over-done. It has its funny parts but its extremely predictable and I didn't want to stay to see the horrible ending. If I could, I would ban these actors and actresses, the only reason being is that Debbie Rochon (Maddy) has been in over a hundred other videos and I've also seen two other members of the cast in equally or worse motion pictures. They should not allowed to continue this madness.\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X[100]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fojp1GeTtKxA"
   },
   "source": [
    "Antes de terminar en la representación final que es conocida como Bag-of-words realizaremos un último preprocesamiento el cual consitirá en eliminar códigos html y caracteres que no sean alfa-numéricos. Para lograr esto usaremos expresiones regulares."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "n05nUqPkvXHq"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "REPLACE_NO_SPACE = re.compile(\"(\\.)|(\\;)|(\\:)|(\\!)|(\\')|(\\?)|(\\,)|(\\\")|(\\()|(\\))|(\\[)|(\\])\")\n",
    "REPLACE_WITH_SPACE = re.compile(\"(<br\\s*/><br\\s*/>)|(\\-)|(\\/)\")\n",
    "\n",
    "def review_to_words(review):\n",
    "    words = REPLACE_NO_SPACE.sub(\"\", review.lower())\n",
    "    words = REPLACE_WITH_SPACE.sub(\" \", words)\n",
    "    return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "id": "8RLLiGinvvfl",
    "outputId": "4f39c420-7a11-48d1-d488-50fe8dd3e177"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'let me being by saying the i followed watching this video by watching saw and after bleed saw looked like the all time greatest horror flick ever even though i thought it was only fairly good bleed is pretty bad the best part is seeing the female cast nude the gore is very fake looking and over done it has its funny parts but its extremely predictable and i didnt want to stay to see the horrible ending if i could i would ban these actors and actresses the only reason being is that debbie rochon maddy has been in over a hundred other videos and ive also seen two other members of the cast in equally or worse motion pictures they should not allowed to continue this madness'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review_to_words(train_X[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "FivQ-bU3vx6D"
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "cache_dir = os.path.join(\"../cache\", \"sentiment_web_app\")  # where to store cache files\n",
    "os.makedirs(cache_dir, exist_ok=True)  # ensure cache directory exists\n",
    "\n",
    "def preprocess_data(data_train, data_test, labels_train, labels_test,\n",
    "                    cache_dir=cache_dir, cache_file=\"preprocessed_data.pkl\"):\n",
    "    \"\"\"Convert each review to words; read from cache if available.\"\"\"\n",
    "\n",
    "    # If cache_file is not None, try to read from it first\n",
    "    cache_data = None\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
    "                cache_data = pickle.load(f)\n",
    "            print(\"Read preprocessed data from cache file:\", cache_file)\n",
    "        except:\n",
    "            pass  # unable to read from cache, but that's okay\n",
    "    \n",
    "    # If cache is missing, then do the heavy lifting\n",
    "    if cache_data is None:\n",
    "        # Preprocess training and test data to obtain words for each review\n",
    "        #words_train = list(map(review_to_words, data_train))\n",
    "        #words_test = list(map(review_to_words, data_test))\n",
    "        words_train = [review_to_words(review) for review in data_train]\n",
    "        words_test = [review_to_words(review) for review in data_test]\n",
    "        \n",
    "        # Write to cache file for future runs\n",
    "        if cache_file is not None:\n",
    "            cache_data = dict(words_train=words_train, words_test=words_test,\n",
    "                              labels_train=labels_train, labels_test=labels_test)\n",
    "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
    "                pickle.dump(cache_data, f)\n",
    "            print(\"Wrote preprocessed data to cache file:\", cache_file)\n",
    "    else:\n",
    "        # Unpack data loaded from cache file\n",
    "        words_train, words_test, labels_train, labels_test = (cache_data['words_train'],\n",
    "                cache_data['words_test'], cache_data['labels_train'], cache_data['labels_test'])\n",
    "    \n",
    "    return words_train, words_test, labels_train, labels_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YKZTJxd-v7_J",
    "outputId": "4eda99dd-7ab3-4d6d-e931-fc521642d50d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read preprocessed data from cache file: preprocessed_data.pkl\n"
     ]
    }
   ],
   "source": [
    "# Preprocess data\n",
    "train_X, test_X, train_y, test_y = preprocess_data(train_X, test_X, train_y, test_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DndKrHjIwVI6"
   },
   "source": [
    "## Alcanzando la representación bag-of-words \n",
    "\n",
    "Usaremos la representación bag-of-words la cual consistirá en asignar a los reviews una estructura matricial con cada columna representando una palabra y por cada review tendra un 1 en la columna respectiva para una palabra dada. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "_qMgX70av-Ma"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. \n",
    "# from sklearn.externals import joblib\n",
    "\n",
    "# Import joblib package directly\n",
    "import joblib\n",
    "# joblib is an enhanced version of pickle that is more efficient for storing NumPy arrays\n",
    "\n",
    "def extract_BoW_features(words_train, words_test, vocabulary_size=5000,\n",
    "                         cache_dir=cache_dir, cache_file=\"bow_features.pkl\"):\n",
    "    \"\"\"Extract Bag-of-Words for a given set of documents, already preprocessed into words.\"\"\"\n",
    "    \n",
    "    # If cache_file is not None, try to read from it first\n",
    "    cache_data = None\n",
    "    if cache_file is not None:\n",
    "        try:\n",
    "            with open(os.path.join(cache_dir, cache_file), \"rb\") as f:\n",
    "                cache_data = joblib.load(f)\n",
    "            print(\"Read features from cache file:\", cache_file)\n",
    "        except:\n",
    "            pass  # unable to read from cache, but that's okay\n",
    "    \n",
    "    # If cache is missing, then do the heavy lifting\n",
    "    if cache_data is None:\n",
    "        # Fit a vectorizer to training documents and use it to transform them\n",
    "        # NOTE: Training documents have already been preprocessed and tokenized into words;\n",
    "        #       pass in dummy functions to skip those steps, e.g. preprocessor=lambda x: x\n",
    "        vectorizer = CountVectorizer(max_features=vocabulary_size)\n",
    "        features_train = vectorizer.fit_transform(words_train).toarray()\n",
    "\n",
    "        # Apply the same vectorizer to transform the test documents (ignore unknown words)\n",
    "        features_test = vectorizer.transform(words_test).toarray()\n",
    "        \n",
    "        # NOTE: Remember to convert the features using .toarray() for a compact representation\n",
    "        \n",
    "        # Write to cache file for future runs (store vocabulary as well)\n",
    "        if cache_file is not None:\n",
    "            vocabulary = vectorizer.vocabulary_\n",
    "            cache_data = dict(features_train=features_train, features_test=features_test,\n",
    "                             vocabulary=vocabulary)\n",
    "            with open(os.path.join(cache_dir, cache_file), \"wb\") as f:\n",
    "                joblib.dump(cache_data, f)\n",
    "            print(\"Wrote features to cache file:\", cache_file)\n",
    "    else:\n",
    "        # Unpack data loaded from cache file\n",
    "        features_train, features_test, vocabulary = (cache_data['features_train'],\n",
    "                cache_data['features_test'], cache_data['vocabulary'])\n",
    "    \n",
    "    # Return both the extracted features as well as the vocabulary\n",
    "    return features_train, features_test, vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yONX6GSdxLmj",
    "outputId": "83c46567-0a3d-42a8-ad14-2f04b6bb18f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Read features from cache file: bow_features.pkl\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Extract Bag of Words features for both training and test datasets\n",
    "train_X, test_X, vocabulary = extract_BoW_features(train_X, test_X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ng7DZ0wwxN-b",
    "outputId": "c300e7c3-ffdd-4afa-b007-5e1a4c2d722b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(25000, 5000)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TK9Sd2m1xX7P",
    "outputId": "67319f9a-68a8-4244-eeab-f13eb68a126b"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "25000/5000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aH3LBu2rxnM3"
   },
   "source": [
    "Por cada palabra en nuestra representación de bag of words tenemos 5 observaciones es por esto que puede haber una ganancia para los modelos que tienen menos parámetros. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aj4nGq9AzHw4"
   },
   "source": [
    "## Modelando los datos\n",
    "\n",
    "Antes de modelar vamos a usar pandas para almacenar los datos que por el momento estan como listas. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "PPfyk_IwxlLj"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Earlier we shuffled the training dataset so to make things simple we can just assign\n",
    "# the first 10 000 reviews to the validation set and use the remaining reviews for training.\n",
    "val_X = pd.DataFrame(train_X[:10000])\n",
    "train_X = pd.DataFrame(train_X[10000:])\n",
    "\n",
    "val_y = pd.DataFrame(train_y[:10000])\n",
    "train_y = pd.DataFrame(train_y[10000:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "GXByeetJ2ZZG"
   },
   "outputs": [],
   "source": [
    "val_X = val_X.to_numpy()\n",
    "train_X = train_X.to_numpy()\n",
    "\n",
    "val_y = val_y.to_numpy()\n",
    "train_y = train_y.to_numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "flhCKEV44cRA"
   },
   "outputs": [],
   "source": [
    "train_y = train_y.flatten()\n",
    "val_y = val_y.flatten()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wXWEtYn2UBB3",
    "outputId": "1302344e-0ff2-434c-9091-b9a8c0dcc9c1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_y.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1eD97FqSzW3j"
   },
   "source": [
    "### Regresión Logística (Sin regularización)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "taJvmjAlSGvb"
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PotLrwUyzPHp",
    "outputId": "0fdfe63b-791b-4a35-a9b8-42ecffe7106c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.872\n",
      "Accuracy 0.829\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "clf = LogisticRegression(random_state=42, max_iter=1000, penalty='none').fit(train_X, train_y)\n",
    "probs = clf.predict_proba(val_X)\n",
    "probs = probs[:,1].flatten()\n",
    "auc = roc_auc_score(val_y, probs)\n",
    "print('AUC: %.3f' % auc)\n",
    "y_pred = clf.predict(val_X)\n",
    "print('Accuracy %.3f' % accuracy_score(val_y, y_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3NHfOxW35YiH"
   },
   "source": [
    "### Análisis discriminante lineal "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CUhed9OQ39xs",
    "outputId": "48910e99-8af4-495b-835e-0e8a87f9d69e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearDiscriminantAnalysis()"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "clf = LinearDiscriminantAnalysis()\n",
    "clf.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4vVKASmFBFy"
   },
   "source": [
    "El anàlisis discriminante no es escalable ya que al calcular $(K-1)\\times(p+1)$ parametros y al tener que calcular la inversa de la matriz de varianzas covarianzas con un orden de complejidad $O(n^3)$. En la siguiente referencia menciona detalles de los cálculos para estimar la matriz de varianzas-covarianzas. Otro problema es la hipótesis de normalidad que no se cumple. \n",
    "\n",
    "https://stats.stackexchange.com/questions/90615/estimating-the-covariance-matrix-in-linear-discriminant-analysis\n",
    "\n",
    "A continuación calcularemos el área bajo la curva roc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AECBNCZMU_zB",
    "outputId": "f080cfb4-b104-4f2b-9723-3c9192eceaf1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.885\n",
      "Accuracy 0.813\n"
     ]
    }
   ],
   "source": [
    "probs = clf.predict_proba(val_X)\n",
    "probs = probs[:,1].flatten()\n",
    "auc = roc_auc_score(val_y, probs)\n",
    "print('AUC: %.3f' % auc)\n",
    "y_pred = clf.predict(val_X)\n",
    "print('Accuracy %.3f' % accuracy_score(val_y, y_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WixxX7caKfwN"
   },
   "source": [
    "### Naive Bayes \n",
    "\n",
    "Usaremos una versión de Naive-Bayes denominada Bernoulli Naive-Bayes. Las siguientes son buenas referencias.\n",
    "\n",
    "https://scikit-learn.org/stable/modules/naive_bayes.html\n",
    "\n",
    "https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.BernoulliNB.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cJeBbvsO6Ddg",
    "outputId": "f5dbe085-eec6-4173-cc6e-06053572a989"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BernoulliNB()"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.naive_bayes import BernoulliNB\n",
    "clf = BernoulliNB()\n",
    "clf.fit(train_X, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ZIFeEkcBLRRl",
    "outputId": "bc96df0d-700a-4dba-ad69-62d4a8b8f5a0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AUC: 0.916\n",
      "Accuracy 0.843\n"
     ]
    }
   ],
   "source": [
    "probs = clf.predict_proba(val_X)\n",
    "probs = probs[:,1].flatten()\n",
    "auc = roc_auc_score(val_y, probs)\n",
    "print('AUC: %.3f' % auc)\n",
    "y_pred = clf.predict(val_X)\n",
    "print('Accuracy %.3f' % accuracy_score(val_y, y_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lj56thNhWZDb"
   },
   "source": [
    "## Conclusiones \n",
    "\n",
    "* El mejor método resulta ser el Bernoulli Naive-Bayes que asume que los predictores son booleanos con lo que este mètodo saca ventaja de las peculiaridades del dataset, a pesar que tiene apenas un parámetro menos que la regresión logistica. \n",
    "\n",
    "* Se puede sacar màs ventaja de estos mètodos usando otra reprensentación como tf-idf la cual puede reducir la dimensionalidad del espacio de predictores.\n",
    "\n",
    "* Una ventaja del Bernoulli Naive-Bayes es que al tener pocos parámetros puede representar una mejora al costo de ponerlo en producción en la nube ya que ocupa menos memoria.  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k9CHMbOZZu31"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "intro_nlp.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
